"use strict";(self.webpackChunksermas_eu_github_io=self.webpackChunksermas_eu_github_io||[]).push([[833],{5037:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"llm/introduction","title":"Introduction","description":"Follow some guidance to configure different LLM providers from those supported.","source":"@site/docs/llm/introduction.md","sourceDirName":"llm","slug":"/llm/introduction","permalink":"/docs/llm/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/sermas-eu/sermas-eu.github.io/sidebar.ts/docs/llm/introduction.md","tags":[],"version":"current","sidebarPosition":0.00002,"frontMatter":{"sidebar_position":0.00002},"sidebar":"tutorialSidebar","previous":{"title":"LLM configuration","permalink":"/docs/category/llm-configuration"},"next":{"title":"Gemini","permalink":"/docs/llm/gemini"}}');var s=i(4848),o=i(8453);const r={sidebar_position:2e-5},l="Introduction",d={},a=[{value:"Providers and models",id:"providers-and-models",level:2},{value:"Configuring the application settings",id:"configuring-the-application-settings",level:2},{value:"Updating the application",id:"updating-the-application",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,s.jsx)(n.p,{children:"Follow some guidance to configure different LLM providers from those supported."}),"\n",(0,s.jsxs)(n.p,{children:["See also ",(0,s.jsx)(n.a,{href:"https://github.com/sermas-eu/sermas-api/blob/main/libs/sermas/sermas.defaults.ts",children:"the default configuration"})," that can be overridden with environment variables."]}),"\n",(0,s.jsxs)(n.p,{children:["If you followed the getting started guide, ensure you have the latest images with ",(0,s.jsx)(n.code,{children:"docker compose pull"})]}),"\n",(0,s.jsx)(n.h2,{id:"providers-and-models",children:"Providers and models"}),"\n",(0,s.jsx)(n.p,{children:"The Toolkit supports different LLM providers for inference and embeddings."}),"\n",(0,s.jsx)(n.p,{children:"The list of supported providers follows"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./antrophic",children:"antrophic"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./gemini",children:"gemini"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./groq",children:"groq"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./huggingface",children:"huggingface"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./mistral",children:"mistral"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./ollama",children:"ollama"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./openai",children:"openai"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Those can be configured in the API ",(0,s.jsx)(n.code,{children:".env"})," and the used in an application configuration."]}),"\n",(0,s.jsxs)(n.p,{children:["A model can be selected following this pattern ",(0,s.jsx)(n.code,{children:"provider/model"})," such as ",(0,s.jsx)(n.code,{children:"openai/gpt-4o"})," or ",(0,s.jsx)(n.code,{children:"gemini/gemini-1.5-pro"})]}),"\n",(0,s.jsx)(n.p,{children:"The model selection happens at inference time by tagging a request with a special label."}),"\n",(0,s.jsx)(n.p,{children:"Current labels are"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"chat"})," chat with the user"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"tools"})," identify a tool (or function call) in a list from the context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"sentiment"})," provide sentiment analysis over text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"tasks"})," identify and contextualize structured tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"intent"})," identify the user intent from a list of options"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"translation"})," translate text between languages"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"configuring-the-application-settings",children:"Configuring the application settings"}),"\n",(0,s.jsxs)(n.p,{children:["In ",(0,s.jsx)(n.code,{children:"settings.yaml"})," or ",(0,s.jsx)(n.code,{children:"app.yaml"})," under settings section add or update the following lines, adapting the models to your needs."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"llm:\n  chat: openai/gpt-4o\n  tools: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n  sentiment: openai/gpt-4o-mini\n  tasks: gemini/gemini-1.5-pro\n# The following are not specified. The Toolkit will use a model from the provider specified in .env as LLM_SERVICE\n#   intent: ...\n#   translation: ...\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The pattern to follow is ",(0,s.jsx)(n.code,{children:"[provider]/[model]"}),". The list of available models is visible in the kiosk UI, opening the left menu, under LLM settings."]}),"\n",(0,s.jsx)(n.p,{children:"In the above configurations, providers can be mixed to obtain the best experience or precision needed for a specific activities."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note"})," If not specified, the Toolkit will use a model from the specified ",(0,s.jsx)(n.code,{children:"LLM_SERVICE"}),". If not set, the default is ",(0,s.jsx)(n.code,{children:"openai"}),"."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["Ensure the default ",(0,s.jsx)(n.code,{children:"LLM_SERVICE"})," is configured or the call to that service will fail!"]})}),"\n",(0,s.jsx)(n.h2,{id:"updating-the-application",children:"Updating the application"}),"\n",(0,s.jsxs)(n.p,{children:["Reimport the app from the CLI ",(0,s.jsx)(n.code,{children:"sermas-cli app save /apps/myapp"})]}),"\n",(0,s.jsxs)(n.p,{children:["Reloading the page at ",(0,s.jsx)(n.a,{href:"http://localhost:8080",children:"http://localhost:8080"})," you can start using the configured model."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);